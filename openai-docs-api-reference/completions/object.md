# The completion object

Represents a completion response from the API. Note: both the streamed and non-streamed response objects share the same shape (unlike the chat endpoint).


## Properties

| Property | Type | Required | Default | Allowed Values | Description |
| -------- | ---- | -------- | ------- | -------------- | ----------- |
| `id` | string | Yes |  |  | A unique identifier for the completion. |
| `choices` | array of object (4 properties) | Yes |  |  | The list of completion choices the model generated for the input prompt. |
| `created` | integer | Yes |  |  | The Unix timestamp (in seconds) of when the completion was created. |
| `model` | string | Yes |  |  | The model used for completion. |
| `system_fingerprint` | string | No |  |  | This fingerprint represents the backend configuration that the model runs with. <br>  <br> Can be used in conjunction with the `seed` request parameter to understand when backend changes have been made that might impact determinism. <br>  |
| `object` | string | Yes |  | `text_completion` | The object type, which is always "text_completion" |
| `usage` | object (5 properties) | No |  |  | Usage statistics for the completion request. |
|   ↳ `total_tokens` | integer | Yes | `0` |  | Total number of tokens used in the request (prompt + completion). |
|   ↳ `completion_tokens_details` | object (4 properties) | No |  |  | Breakdown of tokens used in a completion. |
|     ↳ `reasoning_tokens` | integer | No | `0` |  | Tokens generated by the model for reasoning. |
|     ↳ `rejected_prediction_tokens` | integer | No | `0` |  | When using Predicted Outputs, the number of tokens in the <br> prediction that did not appear in the completion. However, like <br> reasoning tokens, these tokens are still counted in the total <br> completion tokens for purposes of billing, output, and context window <br> limits. <br>  |
|   ↳ `prompt_tokens_details` | object (2 properties) | No |  |  | Breakdown of tokens used in the prompt. |


### Items in `choices` array

| Property | Type | Required | Default | Allowed Values | Description |
| -------- | ---- | -------- | ------- | -------------- | ----------- |
| `finish_reason` | string | Yes |  | `stop`, `length`, `content_filter` | The reason the model stopped generating tokens. This will be `stop` if the model hit a natural stop point or a provided stop sequence, <br> `length` if the maximum number of tokens specified in the request was reached, <br> or `content_filter` if content was omitted due to a flag from our content filters. <br>  |
| `index` | integer | Yes |  |  |  |
| `logprobs` | object (4 properties) | Yes |  |  |  |
|   ↳ `tokens` | array of string | No |  |  |  |
|   ↳ `top_logprobs` | array of object (map of number) | No |  |  |  |
| `text` | string | Yes |  |  |  |

## Property Details

### `id` (required)

A unique identifier for the completion.

**Type**: string

### `choices` (required)

The list of completion choices the model generated for the input prompt.

**Type**: array of object (4 properties)

### `created` (required)

The Unix timestamp (in seconds) of when the completion was created.

**Type**: integer

### `model` (required)

The model used for completion.

**Type**: string

### `system_fingerprint`

This fingerprint represents the backend configuration that the model runs with.

Can be used in conjunction with the `seed` request parameter to understand when backend changes have been made that might impact determinism.


**Type**: string

### `object` (required)

The object type, which is always "text_completion"

**Type**: string

**Allowed values**: `text_completion`

### `usage`

Usage statistics for the completion request.

**Type**: object (5 properties)

**Nested Properties**:

* `completion_tokens`, `prompt_tokens`, `total_tokens`, `completion_tokens_details`, `prompt_tokens_details`

## Example

```json
{
  "id": "cmpl-uqkvlQyYK7bGYrRHQ0eXlWi7",
  "object": "text_completion",
  "created": 1589478378,
  "model": "gpt-4-turbo",
  "choices": [
    {
      "text": "\n\nThis is indeed a test",
      "index": 0,
      "logprobs": null,
      "finish_reason": "length"
    }
  ],
  "usage": {
    "prompt_tokens": 5,
    "completion_tokens": 7,
    "total_tokens": 12
  }
}

```

