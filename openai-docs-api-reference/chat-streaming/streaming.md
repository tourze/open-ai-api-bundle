# The chat completion chunk object

Represents a streamed chunk of a chat completion response returned
by the model, based on the provided input. 
[Learn more](/docs/guides/streaming-responses).


## Properties

| Property | Type | Required | Default | Allowed Values | Description |
| -------- | ---- | -------- | ------- | -------------- | ----------- |
| `id` | string | Yes |  |  | A unique identifier for the chat completion. Each chunk has the same ID. |
| `choices` | array of object (4 properties) | Yes |  |  | A list of chat completion choices. Can contain more than one elements if `n` is greater than 1. Can also be empty for the <br> last chunk if you set `stream_options: {"include_usage": true}`. <br>  |
| `created` | integer | Yes |  |  | The Unix timestamp (in seconds) of when the chat completion was created. Each chunk has the same timestamp. |
| `model` | string | Yes |  |  | The model to generate the completion. |
| `service_tier` | string | No | `auto` | `auto`, `default`, `flex` | Specifies the latency tier to use for processing the request. This parameter is relevant for customers subscribed to the scale tier service: <br>   - If set to 'auto', and the Project is Scale tier enabled, the system <br>     will utilize scale tier credits until they are exhausted. <br>   - If set to 'auto', and the Project is not Scale tier enabled, the request will be processed using the default service tier with a lower uptime SLA and no latency guarentee. <br>   - If set to 'default', the request will be processed using the default service tier with a lower uptime SLA and no latency guarentee. <br>   - If set to 'flex', the request will be processed with the Flex Processing service tier. [Learn more](/docs/guides/flex-processing). <br>   - When not set, the default behavior is 'auto'. <br>  <br>   When this parameter is set, the response body will include the `service_tier` utilized. <br>  |
| `system_fingerprint` | string | No |  |  | This fingerprint represents the backend configuration that the model runs with. <br> Can be used in conjunction with the `seed` request parameter to understand when backend changes have been made that might impact determinism. <br>  |
| `object` | string | Yes |  | `chat.completion.chunk` | The object type, which is always `chat.completion.chunk`. |
| `usage` | object (5 properties) | No |  |  | Usage statistics for the completion request. |
|   ↳ `total_tokens` | integer | Yes | `0` |  | Total number of tokens used in the request (prompt + completion). |
|   ↳ `completion_tokens_details` | object (4 properties) | No |  |  | Breakdown of tokens used in a completion. |
|     ↳ `reasoning_tokens` | integer | No | `0` |  | Tokens generated by the model for reasoning. |
|     ↳ `rejected_prediction_tokens` | integer | No | `0` |  | When using Predicted Outputs, the number of tokens in the <br> prediction that did not appear in the completion. However, like <br> reasoning tokens, these tokens are still counted in the total <br> completion tokens for purposes of billing, output, and context window <br> limits. <br>  |
|   ↳ `prompt_tokens_details` | object (2 properties) | No |  |  | Breakdown of tokens used in the prompt. |


### Items in `choices` array

| Property | Type | Required | Default | Allowed Values | Description |
| -------- | ---- | -------- | ------- | -------------- | ----------- |
| `delta` | object (5 properties) | Yes |  |  | A chat completion delta generated by streamed model responses. |
|   ↳ `tool_calls` | array of object (4 properties) | No |  |  |  |
|   ↳ `role` | string | No |  | `developer`, `system`, `user`, `assistant`, `tool` | The role of the author of this message. |
|   ↳ `refusal` | string | No |  |  | The refusal message generated by the model. |


### Items in `tool_calls` array

| Property | Type | Required | Default | Allowed Values | Description |
| -------- | ---- | -------- | ------- | -------------- | ----------- |
| `index` | integer | Yes |  |  |  |
| `id` | string | No |  |  | The ID of the tool call. |
| `type` | string | No |  | `function` | The type of the tool. Currently, only `function` is supported. |
| `function` | object (2 properties) | No |  |  |  |
| `logprobs` | object (2 properties) | No |  |  | Log probability information for the choice. |


### Items in `content` array

| Property | Type | Required | Default | Allowed Values | Description |
| -------- | ---- | -------- | ------- | -------------- | ----------- |
| `token` | string | Yes |  |  | The token. |
| `logprob` | number | Yes |  |  | The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value `-9999.0` is used to signify that the token is very unlikely. |
| `bytes` | array of integer | Yes |  |  | A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be `null` if there is no bytes representation for the token. |
| `top_logprobs` | array of object (3 properties) | Yes |  |  | List of the most likely tokens and their log probability, at this token position. In rare cases, there may be fewer than the number of requested `top_logprobs` returned. |


### Items in `top_logprobs` array

| Property | Type | Required | Default | Allowed Values | Description |
| -------- | ---- | -------- | ------- | -------------- | ----------- |
| `token` | string | Yes |  |  | The token. |
| `logprob` | number | Yes |  |  | The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value `-9999.0` is used to signify that the token is very unlikely. |
| `bytes` | array of integer | Yes |  |  | A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be `null` if there is no bytes representation for the token. |


### Items in `refusal` array

| Property | Type | Required | Default | Allowed Values | Description |
| -------- | ---- | -------- | ------- | -------------- | ----------- |
| `token` | string | Yes |  |  | The token. |
| `logprob` | number | Yes |  |  | The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value `-9999.0` is used to signify that the token is very unlikely. |
| `bytes` | array of integer | Yes |  |  | A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be `null` if there is no bytes representation for the token. |
| `top_logprobs` | array of object (3 properties) | Yes |  |  | List of the most likely tokens and their log probability, at this token position. In rare cases, there may be fewer than the number of requested `top_logprobs` returned. |


### Items in `top_logprobs` array

| Property | Type | Required | Default | Allowed Values | Description |
| -------- | ---- | -------- | ------- | -------------- | ----------- |
| `token` | string | Yes |  |  | The token. |
| `logprob` | number | Yes |  |  | The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value `-9999.0` is used to signify that the token is very unlikely. |
| `bytes` | array of integer | Yes |  |  | A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be `null` if there is no bytes representation for the token. |
| `finish_reason` | string | Yes |  | `stop`, `length`, `tool_calls`, `content_filter`, `function_call` | The reason the model stopped generating tokens. This will be `stop` if the model hit a natural stop point or a provided stop sequence, <br> `length` if the maximum number of tokens specified in the request was reached, <br> `content_filter` if content was omitted due to a flag from our content filters, <br> `tool_calls` if the model called a tool, or `function_call` (deprecated) if the model called a function. <br>  |
| `index` | integer | Yes |  |  | The index of the choice in the list of choices. |

## Property Details

### `id` (required)

A unique identifier for the chat completion. Each chunk has the same ID.

**Type**: string

### `choices` (required)

A list of chat completion choices. Can contain more than one elements if `n` is greater than 1. Can also be empty for the
last chunk if you set `stream_options: {"include_usage": true}`.


**Type**: array of object (4 properties)

### `created` (required)

The Unix timestamp (in seconds) of when the chat completion was created. Each chunk has the same timestamp.

**Type**: integer

### `model` (required)

The model to generate the completion.

**Type**: string

### `service_tier`

Specifies the latency tier to use for processing the request. This parameter is relevant for customers subscribed to the scale tier service:
  - If set to 'auto', and the Project is Scale tier enabled, the system
    will utilize scale tier credits until they are exhausted.
  - If set to 'auto', and the Project is not Scale tier enabled, the request will be processed using the default service tier with a lower uptime SLA and no latency guarentee.
  - If set to 'default', the request will be processed using the default service tier with a lower uptime SLA and no latency guarentee.
  - If set to 'flex', the request will be processed with the Flex Processing service tier. [Learn more](/docs/guides/flex-processing).
  - When not set, the default behavior is 'auto'.

  When this parameter is set, the response body will include the `service_tier` utilized.


**Type**: string

**Allowed values**: `auto`, `default`, `flex`

**Nullable**: Yes

### `system_fingerprint`

This fingerprint represents the backend configuration that the model runs with.
Can be used in conjunction with the `seed` request parameter to understand when backend changes have been made that might impact determinism.


**Type**: string

### `object` (required)

The object type, which is always `chat.completion.chunk`.

**Type**: string

**Allowed values**: `chat.completion.chunk`

### `usage`

Usage statistics for the completion request.

**Type**: object (5 properties)

**Nullable**: Yes

**Nested Properties**:

* `completion_tokens`, `prompt_tokens`, `total_tokens`, `completion_tokens_details`, `prompt_tokens_details`

## Example

```json
{"id":"chatcmpl-123","object":"chat.completion.chunk","created":1694268190,"model":"gpt-4o-mini", "system_fingerprint": "fp_44709d6fcb", "choices":[{"index":0,"delta":{"role":"assistant","content":""},"logprobs":null,"finish_reason":null}]}

{"id":"chatcmpl-123","object":"chat.completion.chunk","created":1694268190,"model":"gpt-4o-mini", "system_fingerprint": "fp_44709d6fcb", "choices":[{"index":0,"delta":{"content":"Hello"},"logprobs":null,"finish_reason":null}]}

....

{"id":"chatcmpl-123","object":"chat.completion.chunk","created":1694268190,"model":"gpt-4o-mini", "system_fingerprint": "fp_44709d6fcb", "choices":[{"index":0,"delta":{},"logprobs":null,"finish_reason":"stop"}]}

```

